---
title: "Kestrel file combine and clean"
author: "Heather Throop"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---


This code pulls together data from the raw csv files downloaded from the Kestrel Drop2 dataloggers used in the DEAD 1.0 project.

To run this code, all raw Kestrel files should be placed within the "L0" folder for this project on Dropbox. This code pulls data from the Dropbox folder (currently mapped to HT's local drive as API token for Dropbox folder pull not working).

## Libraries and File Input


```{r}
#| label: load needed packages
library(here) # v. 0.1
library(stringr) # v. 1.2.0
library(purrr) # v. 0.2.3
library(tidyverse)
library(scales)
library(lubridate)
```


Display information about package versions in use when this code was run.


```{r}
#| label: session-info
sessionInfo()
```


## Read, Clean, and Combine Original Files

### List Kestrel L0 Files (raw csv files downloaded from Kestrels)

Note that the "more columns than column names" error is likely to happen. Find the offending file (use the last line of code in this chunk to do so) and delete extra columns to the right of the data.\
\
Some of this code is based on the commentary from: <https://aosmith.rbind.io/2017/12/31/many-datasets/>


```{r}
#| label: generate list of files to read in

allfiles <- list.files(
  path = "/Users/hthroop/ASU Dropbox/Heather Throop/DERT (ThroopLab) ASU/Lab Data & Metadata Archives/DEAD_project/logger_data",  
  pattern = "\\.csv$",  
  full.names = TRUE,  
  recursive = TRUE
)

# Exclude files from "zEXTRA_files"
# this folder has some Kestrel files that appear to be redundant as well as some iButton files that will need to be read separately
allfiles <- allfiles[!grepl("zEXTRA_files", allfiles)]

# reading and extract loggerID
read_fun = function(path) {
  test = read.csv(path, 
                  skip = 5,
                  header = FALSE,
                  col.names = c("loggertime", "temperature", "RH", "heatindex", "dewpoint", "datatype") )
  allnames = str_split( path, pattern = "/", simplify = TRUE)
 test$loggerID = str_extract(allnames[, ncol(allnames)], pattern = "2[0-9][0-9][0-9][0-9][0-9][0-9]") #extract the loggerID from the file name and add as a column
  test$RH <- as.numeric(as.character(test$RH))
  test$heatindex <- as.numeric(as.character(test$heatindex))
  test$dewpoint <- as.numeric(as.character(test$dewpoint))
  test$temperature <- as.numeric(as.character(test$temperature))
  test$loggerID <- as.numeric(as.character(test$loggerID))
  test
}

# Line below is useful for testing if logger name is being assigned
# change the number in brackets to check show the top section of each file
#read_fun(allfiles[6]) # use to test the function on an individual file
```


## L1: Combine L0 Files


```{r}
#| label: combine-kestrel-files

# Function to safely read files and handle errors
safe_read_fun <- possibly(read_fun, otherwise = NULL)  

# Combine all valid CSV files into one dataframe
L1_combining <- map_dfr(allfiles, safe_read_fun)

# Remove NULL results (if any files failed to read)
L1_combining <- L1_combining %>% filter(complete.cases(.))

# check the number of sites and loggers in the file
L1_combining |>
  group_by(loggerID) |> 
  summarize(n = n())
```


### Add MetaData

Match the logger ID with the master datasheet that has assignments for site, block, and microsite for each Kestrel


```{r}
#| label: kestrel IDs

# Read in the "KestrelIDs.csv" dataset from the Dropbox link 
KestrelIDs <- read_csv("https://www.dropbox.com/scl/fi/dnpj8xif639dkimbgrzu4/KestrelIDs.csv?rlkey=9ai93kbi3u4jg0aolh9fji4v3&dl=1")

# Perform a left join to merge the additional data based on "loggerID"
L1_merged <- L1_combining |>
  left_join(KestrelIDs, by = "loggerID")

```


The Kestrels change the date and time formatting around in a maddening way. Make the date-time formatting align for all of the lines.


```{r}
#| label: Fix-datetime-formatting 

# make a new file for messing around with date-time formats
L1_cleaning <- L1_merged 

# Initialize a 'datetime' column that will be the correctly formatted datetime
L1_cleaning$datetime <- NA

# Try parsing with flexible parse_date_time function
L1_cleaning$datetime <- parse_date_time(
  L1_cleaning$loggertime, 
  orders = c("mdy HM", "ymd HMS p"),  # List of possible formats
  quiet = FALSE  # This will show warnings and errors if parsing fails
)

# Check if there are still NA values after parsing
if (any(is.na(L1_cleaning$datetime))) {
  cat("Some entries could not be parsed!\n")
  # Print rows where the time column is still NA
  print(L1_cleaning[is.na(L1_cleaning$datetime),])
}
## there is one file from logger 2593668 (DBG) that did not include dates. No worries as these logs all appear to be duplicated in a later file. 

# Remove rows with NA time values (logger 2593668)
L1_cleaning <- L1_cleaning[!is.na(L1_cleaning$datetime), ]

# Ensure the time is in 24-hour format (POSIXct will handle it correctly)
L1_cleaning$datetime <- as.POSIXct(L1_cleaning$datetime, format = "%Y-%m-%d %H:%M:%S")

# Round down to the nearest minute (sets seconds to :00)
L1_cleaning$datetime <- floor_date(L1_cleaning$datetime, unit = "minute")

# This is the completed L1 file
L1_clean <- L1_cleaning

#save the L1 output as a .csv file into the DEAD Dropbox
write.csv(L1_clean, file = file.path(
  "/Users/hthroop/ASU Dropbox/Heather Throop/DERT (ThroopLab) ASU/Lab Data & Metadata Archives/DEAD_project/logger_data/L1data", 
  "DEAD_Kestrel_L1.csv"
))
```


## L2: Data Cleaned of Extra Lines


```{r}
#| label: Filter to remove extra lines

# Move on to Level 2 cleaning
L2_cleaning <- L1_clean

# Remove any duplicate lines - there are many of these due to re-downloading the same loggers and not cleaning off old data in between downloads
L2_cleaning <- L2_cleaning |>
  distinct()

# remove any data before the deployment date, using starttime and startdate from KestrelIDs that correspond with each loggerID 
L2 <- L2_cleaning |>
  mutate(start_datetime = as.POSIXct(paste(startdate, starttime), 
                                     format = "%m/%d/%Y %H:%M")) |>
  filter(datetime >= start_datetime) |>
  select(-startdate, -starttime, -loggertime) 

# Check that all loggers are still present
L2 |>
  group_by(loggerID) |> 
  summarize(n = n())

# reorder columns and get rid of extra columns (including heatindex and dewpoint)
L2 <- L2 |>
  select(-datatype, -start_datetime, -heatindex, -dewpoint)|>
  select(datetime, site, block, microsite, cluster, loggerID, everything())  # Reorder columns

#save the L2 output as a .csv file into the DEAD Dropbox
write.csv(L2, file = file.path(
  "/Users/hthroop/ASU Dropbox/Heather Throop/DERT (ThroopLab) ASU/Lab Data & Metadata Archives/DEAD_project/logger_data/L2data", 
  "DEAD_Kestrel_L2.csv"
))
```


### Plots: Individual Loggers

Plot out each individual logger to check for missing data or other possibly data issues


```{r}
#| label: DBG plots

DBG_RH_by_logger <- L2 |>
  subset(site=="DBG") |> 
  ggplot(aes(x=datetime, y=RH, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "DBG") +
    xlab("Date") +
    ylab("Relative Humidity (%)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
DBG_RH_by_logger

DBG_T_by_logger <- L2 |>
  subset(site=="DBG") |> 
  ggplot(aes(x=datetime, y=temperature, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "DBG") +
    xlab("Date") +
    ylab("Temperature (C)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
DBG_T_by_logger

```

```{r}
#| label: JER plots

JER_RH_by_logger <- L2 |>
  subset(site=="JER") |> 
  ggplot(aes(x=datetime, y=RH, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "JER") +
    xlab("Date") +
    ylab("Relative Humidity (%)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
JER_RH_by_logger

JER_T_by_logger <- L2 |>
  subset(site=="JER") |> 
  ggplot(aes(x=datetime, y=temperature, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "JER") +
    xlab("Date") +
    ylab("Temperature (C)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
JER_T_by_logger

```

```{r}
#| label: Moab plots

Moab_RH_by_logger <- L2 |>
  subset(site=="Moab") |> 
  ggplot(aes(x=datetime, y=RH, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "Moab") +
    xlab("Date") +
    ylab("Relative Humidity (%)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
Moab_RH_by_logger

Moab_T_by_logger <- L2 |>
  subset(site=="Moab") |> 
  ggplot(aes(x=datetime, y=temperature, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "Moab") +
    xlab("Date") +
    ylab("Temperature (C)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
Moab_T_by_logger
```

```{r}
#| label: MOJ plots

MOJ_RH_by_logger <- L2 |>
  subset(site=="MOJ") |> 
  ggplot(aes(x=datetime, y=RH, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "MOJ") +
    xlab("Date") +
    ylab("Relative Humidity (%)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
MOJ_RH_by_logger

MOJ_T_by_logger <- L2 |>
  subset(site=="MOJ") |> 
  ggplot(aes(x=datetime, y=temperature, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "MOJ") +
    xlab("Date") +
    ylab("Temperature (C)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
MOJ_T_by_logger
```

```{r}
#| label: RCEW plots

RCEW_RH_by_logger <- L2 |>
  subset(site=="RCEW") |> 
  ggplot(aes(x=datetime, y=RH, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "RCEW") +
    xlab("Date") +
    ylab("Relative Humidity (%)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
RCEW_RH_by_logger

RCEW_T_by_logger <- L2 |>
  subset(site=="RCEW") |> 
  ggplot(aes(x=datetime, y=temperature, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "RCEW") +
    xlab("Date") +
    ylab("Temperature (C)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
RCEW_T_by_logger
```

