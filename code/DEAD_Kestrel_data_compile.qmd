---
title: "Kestrel file combine and clean"
author: "Heather Throop"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

This code pulls together data from the raw csv files downloaded from the Kestrel Drop2 dataloggers used in the DEAD 1.0 project. It creates a compiled L1 file with all data combined and a cleaned L2 file with duplicates and extraneous data removed.These files are written to the Dropbox folder for this project.

This code ingests the raw Kestrel csv files that are in the "L0raw_data" folder for this project on Dropbox. This code pulls data from the Dropbox folder using a config.yml file that maps to the Dropbox folder ("\~/DeAD NSF MSB Dryland Decomposition/Field&LabCircle/DEAD1_0/logger_data"). Data are written the same way.



```{r}
#| label: load needed packages
library(stringr) # v. 1.2.0
library(purrr) # v. 0.2.3
library(tidyverse)
library(scales)
library(lubridate)
```

Display information about package versions in use when this code was run.

```{r}
#| label: session-info
sessionInfo()
```

```{r}
# Use the stored config file to define the path for the DeAD Dropbox folder 
DEADloggers <- config::get()
```

## Read, Clean, and Combine Original Files

### List Kestrel L0 Files (raw csv files downloaded from Kestrels)

Note that the "more columns than column names" error is likely to happen. Find the offending file (use the last line of code in this chunk to do so) and delete extra columns to the right of the data.\
\
Some of this code is based on the commentary from: <https://aosmith.rbind.io/2017/12/31/many-datasets/>

```{r}
#| label: generate list of files to read in

allfiles <- list.files(
  path = file.path(DEADloggers, "L0raw_data"),
  pattern = "\\.csv$",
  full.names = TRUE,
  recursive = TRUE
)

# Exclude files from "zEXTRA_files"
# this folder has some Kestrel files that appear to be redundant as well as some iButton files that will need to be read separately
allfiles <- allfiles[!grepl("zEXTRA_files", allfiles)]

# reading and extract loggerID
read_fun = function(path) {
  test = read.csv(path, 
                  skip = 5,
                  header = FALSE,
                  col.names = c("loggertime", "temperature", "RH", "heatindex", "dewpoint", "datatype") )
  allnames = str_split( path, pattern = "/", simplify = TRUE)
 test$loggerID = str_extract(allnames[, ncol(allnames)], pattern = "2[0-9][0-9][0-9][0-9][0-9][0-9]") #extract the loggerID from the file name and add as a column
  test$RH <- as.numeric(as.character(test$RH))
  test$heatindex <- as.numeric(as.character(test$heatindex))
  test$dewpoint <- as.numeric(as.character(test$dewpoint))
  test$temperature <- as.numeric(as.character(test$temperature))
  test$loggerID <- as.numeric(as.character(test$loggerID))
  test
}

# Line below is useful for testing if logger name is being assigned
# change the number in brackets to check show the top section of each file
read_fun(allfiles[81]) # use to test the function on an individual file
```

## L1: Combine L0 Files

```{r}
#| label: combine-kestrel-files

# Function to safely read files and handle errors
safe_read_fun <- possibly(read_fun, otherwise = NULL)  

# Combine all valid CSV files into one dataframe
L1_combining <- map_dfr(allfiles, safe_read_fun)

# Remove NULL results (if any files failed to read)
L1_combining <- L1_combining %>% filter(complete.cases(.))

# check the number of sites and loggers in the file
L1_combining |>
  group_by(loggerID) |> 
  summarize(n = n())
```

### Add MetaData

Match the logger ID with the master datasheet that has assignments for site, block, and microsite for each Kestrel

```{r}
#| label: kestrel IDs

# Read in the "KestrelIDs.csv" dataset from the Dropbox link 
KestrelIDs <- read_csv(file.path(DEADloggers, "KestrelIDs.csv"))

# Perform a left join to merge the additional data based on "loggerID"
L1_merged <- L1_combining |>
  left_join(KestrelIDs, by = "loggerID")

```

The Kestrels change the date and time formatting around in a maddening way. Make the date-time formatting align for all of the lines.

```{r}
#| label: Fix-datetime-formatting 

# make a new file for messing around with date-time formats
L1_cleaning <- L1_merged 

# define a whole mess of possible date-time formats
orders <- c(
  "mdy HM",            # e.g. 7/18/2021 13:00
  "mdy HMS",           # e.g. 7/18/2021 13:00:00
  "ymd HMS p",         # e.g. 2021-07-18 01:00:00 PM
  "Y-m-d H:M:S",       # e.g. 2021-07-18 13:00:00
  "b d, Y H:M:S",      # e.g. Jul 18, 2021 13:00:00  
  "b d, Y I:M:S p",    # e.g. Jul 18, 2021 01:00:00 PM
  "b d Y H:M:S"        # e.g. Jul 18 2021 13:00:00 (no comma)
)

# Try parsing with multiple possible formats
L1_cleaning <- L1_cleaning |>
  mutate(
    datetime = parse_date_time(
      loggertime, 
      orders = c("mdy HM", "ymd HMS p", "b d, Y I:M:S p"),  # add more formats if necessary
      tz = "UTC",  # Adjust if necessary
      quiet = FALSE  # Show warnings if parsing fails
    )
  )

L1_cleaning <- L1_cleaning |>
  mutate(
    # trim whitespace first
    loggertime_trim = trimws(as.character(loggertime)),
    # primary parsing attempt using lubridate
    datetime = parse_date_time(loggertime_trim, orders = orders, tz = "UTC", quiet = TRUE),
    # fallback: try a strict POSIXct parse for the specific format "Jul 18, 2021 13:00:00"
    datetime = coalesce(
      datetime,
      as.POSIXct(loggertime_trim, format = "%b %d, %Y %H:%M:%S", tz = "UTC"),
      # final fallback: try a more permissive anytime-like parse if you have anytime installed
      # anytime::anytime(loggertime_trim, tz = "UTC")
    )
  ) |>
  select(-loggertime_trim)   # drop helper column 


# Check if there are still NA values after parsing
if (any(is.na(L1_cleaning$datetime))) {
  cat("Some entries could not be parsed!\n")
  # Print rows where the time column is still NA
  print(L1_cleaning[is.na(L1_cleaning$datetime),])
}
## there is one file from logger 2593668 (DBG) that did not include dates. No worries as these logs all appear to be duplicated in a later file. 

# Remove rows with NA time values (logger 2593668)
L1_cleaning <- L1_cleaning[!is.na(L1_cleaning$datetime), ]

# Ensure the time is in 24-hour format (POSIXct will handle it correctly)
L1_cleaning$datetime <- as.POSIXct(L1_cleaning$datetime, format = "%Y-%m-%d %H:%M:%S")

# Round down to the nearest minute (sets seconds to :00)
L1_cleaning$datetime <- floor_date(L1_cleaning$datetime, unit = "minute")

# This is the completed L1 file
L1_clean <- L1_cleaning

#save the L1 output as a .csv file into the Dropbox
write.csv(L1_clean, file = file.path(DEADloggers,  "DEAD_Kestrel_L1.csv"))
```

## L2: Data Cleaned of Extra Lines

```{r}
#| label: Filter to remove extra lines

# Move on to Level 2 cleaning
L2_cleaning <- L1_clean

# Remove any duplicate lines - there are many of these due to re-downloading the same loggers and not cleaning off old data in between downloads
L2_cleaning <- L2_cleaning |>
  distinct()

# remove any data before the deployment date and remove random extra logs that sometimes occur after the logger has been nonfunctional for a while. These are based on enddate, endtime, starttime, and startdate from KestrelID 

L2 <- L2_cleaning |>
  mutate(
    startdate = as.character(startdate),  
    starttime = as.character(starttime),  
    enddate = as.character(enddate),  
    endtime = as.character(endtime),  
    start_datetime = as.POSIXct(paste(startdate, starttime), 
                                format = "%m/%d/%y %H:%M:%S", tz = "UTC"),
    end_datetime = as.POSIXct(paste(enddate, endtime), 
                              format = "%m/%d/%y %H:%M:%S", tz = "UTC")) |>
  filter(!is.na(start_datetime) & !is.na(end_datetime) & 
          datetime >= start_datetime & datetime <= end_datetime)

# Check that all loggers are still present
L2 |>
  group_by(loggerID) |> 
  summarize(n = n())

# reorder columns and get rid of extra columns (including heatindex and dewpoint)
L2 <- L2 |>
  select(-datatype, -start_datetime, -heatindex, -dewpoint, 
         -startdate, -starttime, -loggertime)|>
  select(datetime, site, block, microsite, cluster, loggerID, everything())  

#save the L2 output as a .csv file into the Dropbox
write.csv(L2, file = file.path(DEADloggers,  "DEAD_Kestrel_L2.csv"))
```

### Plots: Individual Loggers

Plot out each individual logger to check for missing data or other possibly data issues

```{r}
#| label: JER plots

JER_RH_by_logger <- L2 |>
  subset(site=="JER") |> 
  ggplot(aes(x=datetime, y=RH, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "JER") +
    xlab("Date") +
    ylab("Relative Humidity (%)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
JER_RH_by_logger

JER_T_by_logger <- L2 |>
  subset(site=="JER") |> 
  ggplot(aes(x=datetime, y=temperature, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "JER") +
    xlab("Date") +
    ylab("Temperature (C)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
JER_T_by_logger

```

```{r}
#| label: Moab plots

Moab_RH_by_logger <- L2 |>
  subset(site=="Moab") |> 
  ggplot(aes(x=datetime, y=RH, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "Moab") +
    xlab("Date") +
    ylab("Relative Humidity (%)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
Moab_RH_by_logger

Moab_T_by_logger <- L2 |>
  subset(site=="Moab") |> 
  ggplot(aes(x=datetime, y=temperature, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "Moab") +
    xlab("Date") +
    ylab("Temperature (C)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
Moab_T_by_logger
```

```{r}
#| label: MOJ plots

MOJ_RH_by_logger <- L2 |>
  subset(site=="MOJ") |> 
  ggplot(aes(x=datetime, y=RH, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "MOJ") +
    xlab("Date") +
    ylab("Relative Humidity (%)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
MOJ_RH_by_logger

MOJ_T_by_logger <- L2 |>
  subset(site=="MOJ") |> 
  ggplot(aes(x=datetime, y=temperature, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "MOJ") +
    xlab("Date") +
    ylab("Temperature (C)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
MOJ_T_by_logger
```

```{r}
#| label: RCEW plots

RCEW_RH_by_logger <- L2 |>
  subset(site=="RCEW") |> 
  ggplot(aes(x=datetime, y=RH, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "RCEW") +
    xlab("Date") +
    ylab("Relative Humidity (%)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
RCEW_RH_by_logger

RCEW_T_by_logger <- L2 |>
  subset(site=="RCEW") |> 
  ggplot(aes(x=datetime, y=temperature, color = microsite, group = 1)) + 
    geom_line() +
    labs(title = "RCEW") +
    xlab("Date") +
    ylab("Temperature (C)") +
    scale_x_datetime(date_breaks = "2 months", labels = date_format("%b %Y")) +
    facet_grid(loggerID ~ .)
RCEW_T_by_logger
```



